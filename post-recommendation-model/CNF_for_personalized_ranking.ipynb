{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Load Data\n",
        "users_df = pd.read_csv(\"users.csv\")\n",
        "posts_df = pd.read_csv(\"posts.csv\")\n",
        "interactions_df = pd.read_csv(\"interactions.csv\")\n",
        "\n",
        "# Fix Indexing: Map user_id and post_id to sequential indices\n",
        "user_mapping = {id: idx for idx, id in enumerate(users_df[\"user_id\"].unique())}\n",
        "post_mapping = {id: idx for idx, id in enumerate(posts_df[\"post_id\"].unique())}\n",
        "\n",
        "interactions_df[\"user_id\"] = interactions_df[\"user_id\"].map(user_mapping)\n",
        "interactions_df[\"post_id\"] = interactions_df[\"post_id\"].map(post_mapping)\n",
        "\n",
        "# Ensure IDs are within range (avoid index errors)\n",
        "n_users = len(user_mapping)\n",
        "n_posts = len(post_mapping)\n",
        "\n",
        "# Convert implicit feedback into ratings\n",
        "interactions_df[\"rating\"] = (\n",
        "    interactions_df[\"upvotes\"] * 1 + interactions_df[\"saves\"] * 3 - interactions_df[\"downvotes\"] * 2\n",
        ")\n",
        "\n",
        "# Define PyTorch Dataset\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = torch.tensor(df[\"user_id\"].values, dtype=torch.long)\n",
        "        self.posts = torch.tensor(df[\"post_id\"].values, dtype=torch.long)\n",
        "        self.ratings = torch.tensor(df[\"rating\"].values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.posts[idx], self.ratings[idx]\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = InteractionDataset(interactions_df)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define NCF Model\n",
        "class NCF(nn.Module):\n",
        "    def __init__(self, n_users, n_posts, embedding_dim=32):\n",
        "        super(NCF, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.post_embedding = nn.Embedding(n_posts, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user, post):\n",
        "        user_embedded = self.user_embedding(user)\n",
        "        post_embedded = self.post_embedding(post)\n",
        "        x = torch.cat([user_embedded, post_embedded], dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x.squeeze()\n",
        "\n",
        "# Initialize Model\n",
        "model = NCF(n_users, n_posts)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, dataloader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for users, posts, ratings in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(users, posts)\n",
        "            loss = criterion(predictions, ratings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Train Model\n",
        "train_model(model, dataloader)\n",
        "\n",
        "# Generate NCF-based recommendations\n",
        "def get_ncf_recommendations(user_id, top_n=50):\n",
        "    if user_id not in user_mapping:\n",
        "        return []\n",
        "    user_idx = user_mapping[user_id]\n",
        "    post_ids = list(post_mapping.keys())\n",
        "    predictions = [(post, model(torch.tensor([user_idx]), torch.tensor([post_mapping[post]])).item()) for post in post_ids]\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [post for post, _ in predictions[:top_n]]\n",
        "\n",
        "# Content-Based Filtering (TF-IDF / Similarity)\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "tfidf_matrix = tfidf.fit_transform(posts_df[\"content\"])\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "def get_content_based_recommendations(user_id, recommendations, top_n=30):\n",
        "    sim_scores = cosine_sim[[post_mapping[post] for post in recommendations]].mean(axis=0)\n",
        "    post_indices = np.argsort(sim_scores)[::-1][:top_n]\n",
        "    return posts_df.iloc[post_indices][\"post_id\"].tolist()\n",
        "\n",
        "# Industry & Work Profile Prioritization\n",
        "def get_industry_recommendations(user_id, recommendations, top_n=20):\n",
        "    user_industry = users_df[users_df[\"user_id\"] == user_id][\"industry\"].values[0]\n",
        "    industry_posts = posts_df[posts_df[\"industry\"] == user_industry][\"post_id\"].tolist()\n",
        "    prioritized = [post for post in recommendations if post in industry_posts]\n",
        "    remaining = [post for post in recommendations if post not in industry_posts]\n",
        "    return (prioritized + remaining)[:top_n]\n",
        "\n",
        "# Bayesian Ranking (Wilson Score Interval)\n",
        "def wilson_score(upvotes, downvotes, confidence=0.95):\n",
        "    n = upvotes + downvotes\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    z = norm.ppf(1 - (1 - confidence) / 2)\n",
        "    p = upvotes / n\n",
        "    return (p + z**2 / (2 * n) - z * ((p * (1 - p) + z**2 / (4 * n)) / n)**0.5) / (1 + z**2 / n)\n",
        "\n",
        "posts_df[\"wilson_score\"] = posts_df.apply(lambda row: wilson_score(row[\"upvotes\"], row[\"downvotes\"]), axis=1)\n",
        "posts_df = posts_df.sort_values(by=\"wilson_score\", ascending=False)\n",
        "\n",
        "def get_final_recommendations(user_id, top_n=10):\n",
        "    ncf_recs = get_ncf_recommendations(user_id, top_n=50)\n",
        "    content_recs = get_content_based_recommendations(user_id, ncf_recs, top_n=30)\n",
        "    industry_recs = get_industry_recommendations(user_id, content_recs, top_n=20)\n",
        "    final_recommendations = sorted(industry_recs, key=lambda post: posts_df[posts_df[\"post_id\"] == post][\"wilson_score\"].values[0], reverse=True)\n",
        "    return final_recommendations[:top_n]\n",
        "\n",
        "user_id = 5\n",
        "recommended_posts = get_final_recommendations(user_id, top_n=10)\n",
        "print(\"Recommended Posts:\", recommended_posts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haL1JrapH8oX",
        "outputId": "430c500f-bde5-4577-ea9c-2c1d454c6588"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 3.5551\n",
            "Epoch 2/5, Loss: 3.4543\n",
            "Epoch 3/5, Loss: 3.4358\n",
            "Epoch 4/5, Loss: 3.4189\n",
            "Epoch 5/5, Loss: 3.3650\n",
            "Recommended Posts: [302, 4593, 4219, 2761, 3926, 2705, 4943, 1308, 2483, 3576]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Load Data\n",
        "users_df = pd.read_csv(\"users.csv\")\n",
        "posts_df = pd.read_csv(\"posts.csv\")\n",
        "interactions_df = pd.read_csv(\"interactions.csv\")\n",
        "\n",
        "# Fix Indexing: Map user_id and post_id to sequential indices\n",
        "user_mapping = {id: idx for idx, id in enumerate(users_df[\"user_id\"].unique())}\n",
        "post_mapping = {id: idx for idx, id in enumerate(posts_df[\"post_id\"].unique())}\n",
        "\n",
        "interactions_df[\"user_id\"] = interactions_df[\"user_id\"].map(user_mapping)\n",
        "interactions_df[\"post_id\"] = interactions_df[\"post_id\"].map(post_mapping)\n",
        "\n",
        "# Ensure IDs are within range (avoid index errors)\n",
        "n_users = len(user_mapping)\n",
        "n_posts = len(post_mapping)\n",
        "\n",
        "# Convert implicit feedback into ratings\n",
        "interactions_df[\"rating\"] = (\n",
        "    interactions_df[\"upvotes\"] * 1 + interactions_df[\"saves\"] * 3 - interactions_df[\"downvotes\"] * 2\n",
        ")\n",
        "\n",
        "# Define PyTorch Dataset\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = torch.tensor(df[\"user_id\"].values, dtype=torch.long)\n",
        "        self.posts = torch.tensor(df[\"post_id\"].values, dtype=torch.long)\n",
        "        self.ratings = torch.tensor(df[\"rating\"].values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.posts[idx], self.ratings[idx]\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = InteractionDataset(interactions_df)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define NCF Model\n",
        "class NCF(nn.Module):\n",
        "    def __init__(self, n_users, n_posts, embedding_dim=32):\n",
        "        super(NCF, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.post_embedding = nn.Embedding(n_posts, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user, post):\n",
        "        user_embedded = self.user_embedding(user)\n",
        "        post_embedded = self.post_embedding(post)\n",
        "        x = torch.cat([user_embedded, post_embedded], dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x.squeeze()\n",
        "\n",
        "# Initialize Model\n",
        "model = NCF(n_users, n_posts)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, dataloader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for users, posts, ratings in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(users, posts)\n",
        "            loss = criterion(predictions, ratings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Train Model\n",
        "train_model(model, dataloader)\n",
        "\n",
        "# Generate NCF-based recommendations\n",
        "def get_ncf_recommendations(user_id, top_n=50):\n",
        "    if user_id not in user_mapping:\n",
        "        return []\n",
        "    user_idx = user_mapping[user_id]\n",
        "    post_ids = list(post_mapping.keys())\n",
        "    predictions = [(post, model(torch.tensor([user_idx]), torch.tensor([post_mapping[post]])).item()) for post in post_ids]\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [post for post, _ in predictions[:top_n]]\n",
        "\n",
        "# Content-Based Filtering (TF-IDF / Similarity)\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "tfidf_matrix = tfidf.fit_transform(posts_df[\"content\"])\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "def get_content_based_recommendations(user_id, recommendations, top_n=30):\n",
        "    sim_scores = cosine_sim[[post_mapping[post] for post in recommendations]].mean(axis=0)\n",
        "    post_indices = np.argsort(sim_scores)[::-1][:top_n]\n",
        "    return posts_df.iloc[post_indices][\"post_id\"].tolist()\n",
        "\n",
        "# Industry & Work Profile Prioritization\n",
        "def get_industry_recommendations(user_id, recommendations, top_n=20):\n",
        "    user_industry = users_df[users_df[\"user_id\"] == user_id][\"industry\"].values[0]\n",
        "    industry_posts = posts_df[posts_df[\"industry\"] == user_industry][\"post_id\"].tolist()\n",
        "    prioritized = [post for post in recommendations if post in industry_posts]\n",
        "    remaining = [post for post in recommendations if post not in industry_posts]\n",
        "    return (prioritized + remaining)[:top_n]\n",
        "\n",
        "# Bayesian Ranking (Wilson Score Interval)\n",
        "def wilson_score(upvotes, downvotes, confidence=0.95):\n",
        "    n = upvotes + downvotes\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    z = norm.ppf(1 - (1 - confidence) / 2)\n",
        "    p = upvotes / n\n",
        "    return (p + z**2 / (2 * n) - z * ((p * (1 - p) + z**2 / (4 * n)) / n)**0.5) / (1 + z**2 / n)\n",
        "\n",
        "posts_df[\"wilson_score\"] = posts_df.apply(lambda row: wilson_score(row[\"upvotes\"], row[\"downvotes\"]), axis=1)\n",
        "posts_df = posts_df.sort_values(by=\"wilson_score\", ascending=False)\n",
        "\n",
        "def get_final_recommendations(user_id, top_n=10):\n",
        "    ncf_recs = get_ncf_recommendations(user_id, top_n=50)\n",
        "    content_recs = get_content_based_recommendations(user_id, ncf_recs, top_n=30)\n",
        "    industry_recs = get_industry_recommendations(user_id, content_recs, top_n=20)\n",
        "    final_recommendations = sorted(industry_recs, key=lambda post: posts_df[posts_df[\"post_id\"] == post][\"wilson_score\"].values[0], reverse=True)\n",
        "    return final_recommendations[:top_n]\n",
        "\n",
        "user_id = 5\n",
        "recommended_posts = get_final_recommendations(user_id, top_n=10)\n",
        "print(\"Recommended Posts:\", recommended_posts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFcBY3RlNzdD",
        "outputId": "19969496-0e5a-4504-e5f5-447066e3681b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended Posts: [2080, 1502, 4852, 4595, 3908, 779, 3043, 4019, 4591, 4403]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKPiHJ_4OeNI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}